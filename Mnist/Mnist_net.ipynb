{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./data'\n",
    "TEACHER_PATH=\"mnist_cnn.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train = MNIST(DATA_PATH, train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST(DATA_PATH, train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Definicion y entrenamiento CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(32 * 3 * 3, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train_model=False\n",
    "if train_model:\n",
    "    net.train()\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Get Samples\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            data=data.to(device)\n",
    "            target=target.to(device)\n",
    "            # Init\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict\n",
    "            y_pred = net(data) \n",
    "\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(y_pred, target)\n",
    "            losses.append(loss)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # Display\n",
    "            if batch_idx % 100 == 1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx * len(data), \n",
    "                    len(train_loader.dataset),\n",
    "\n",
    "                    100. * batch_idx / len(train_loader), \n",
    "                    loss), \n",
    "                    end='')\n",
    "    save_model=True\n",
    "    if (save_model):\n",
    "        torch.save(net.state_dict(),PATH)\n",
    "else:\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,loader, lim=10):\n",
    "    d=[]\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "\n",
    "        output = net(data )\n",
    "        pred = output.data.max(1)[1]\n",
    "        d.extend( pred.eq(target).cpu() )\n",
    "        if batch_idx>lim:\n",
    "            continue\n",
    "\n",
    "    d=np.array(d)\n",
    "    accuracy = float(np.sum(d) ) / float( d.shape[0])\n",
    "    return accuracy\n",
    "    \n",
    "test(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destilacion de logits, MLP 1 capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# todo: generalizar input ax,dict\n",
    "def plot_exp(exps, one_lay=True,f_size=10):\n",
    "  mpl.style.use('seaborn')\n",
    "  c=plt.get_cmap('tab20')\n",
    "  \n",
    "  terminal=[]\n",
    "  y_keys=list(exps[2].keys())# todo:arreglar\n",
    "  y_keys.remove(\"epoch\")\n",
    "\n",
    "  f, ax = plt.subplots(1, len(y_keys)+int(one_lay), figsize=(f_size*(len(y_keys)+int(one_lay)),f_size))\n",
    "\n",
    "  j=0\n",
    "  for mod,history in exps.items():\n",
    "    for i,k in enumerate(y_keys):\n",
    "      if k !=\"epoch\":\n",
    "        ax[i].semilogy(history[\"epoch\"],history[k], label=mod,color=c(j/20))\n",
    "    terminal.append([int(mod),history[\"train\"][-1],history[\"test\"][-1] ] )\n",
    "    j+=1\n",
    "    \n",
    "  ax[1].legend()\n",
    "\n",
    "  for i,k in enumerate(y_keys):\n",
    "      if k !=\"epoch\":\n",
    "        ax[i].set_title(k)\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].set_ylabel(k)\n",
    "        ax[i].legend()\n",
    "  \n",
    "\n",
    "  if one_lay:\n",
    "    ax[-1].loglog([i[0] for i in terminal],[i[1] for i in terminal], label=\"Train\",basex=2)\n",
    "    ax[-1].loglog([i[0] for i in terminal],[i[2] for i in terminal], label=\"Test\",basex=2)\n",
    "    ax[-1].semilogx([i[0] for i in terminal],[i[2]/i[1] for i in terminal], label=\"test/train\",basex=2)\n",
    "\n",
    "    ax[-1].set_title(\"terminal values\")\n",
    "    ax[-1].set_ylabel(\"C.E. loss\")\n",
    "    ax[-1].set_xlabel(\"hidden size\")\n",
    "\n",
    "    ax[-1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_loss_gen(T=8):\n",
    "  def dist_loss(student_scores,  teacher_scores, T=T):\n",
    "      return nn.KLDivLoss()(F.log_softmax(student_scores/T,dim=1), F.softmax(teacher_scores/T,dim=1))\n",
    "  return dist_loss\n",
    "\n",
    "def sample(loader):\n",
    "    data,target=next(iter(loader))\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    return data,target\n",
    "  \n",
    "def dist_model(T_model, S_model, epochs, criterion, eval_criterion, optimizer):\n",
    "  global train_loader, test_loader\n",
    "  #train_loader, x_test,y_test = experiment_data()\n",
    "  history={\"epoch\":[],\n",
    "           \"train\":[],\n",
    "           \"test\":[],\n",
    "           \"loss\":[],\n",
    "           \"acc_train\":[],\n",
    "           \"acc_test\":[]\n",
    "          }\n",
    "  \n",
    "  S_model.train()\n",
    "  T_model.eval()\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):# 784= 28*28\n",
    "        x_train, y_train = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        \n",
    "        # Predecir\n",
    "        S_y_pred = S_model(x_train.view(-1,784))\n",
    "        T_y_pred = T_model(x_train)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(S_y_pred,T_y_pred)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    if epoch % 1 == 0:\n",
    "        x_train, y_train= sample(train_loader)\n",
    "        y_pred = S_model(x_train.view(-1,784))\n",
    "        train_stats = eval_criterion(y_pred, y_train) \n",
    "        \n",
    "        x_test, y_test= sample(test_loader)\n",
    "        y_pred = S_model(x_test.view(-1,784))\n",
    "        test_stats = eval_criterion(y_pred.squeeze(), y_test) \n",
    "        y_predT = T_model(x_test)\n",
    "        test_statsT = eval_criterion(y_predT.squeeze(), y_test) \n",
    "\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        history[\"test\"].append(test_stats.item())\n",
    "        history[\"train\"].append(train_stats.item())\n",
    "        history[\"acc_test\"].append(test(S_model, train_loader))\n",
    "        history[\"acc_train\"].append(test(S_model, test_loader))\n",
    "        #print('Epoch {}: train loss: {}, test loss: {}'.format(epoch, loss.item(), test_stats.item()) )\n",
    "\n",
    "  return history\n",
    "\n",
    "\n",
    "\n",
    "def distillation_experiment(neuronas, epochs, temp, teacher, experiments=2):\n",
    "  exps={}\n",
    "  dist_models={}\n",
    "  \n",
    "  for i in neuronas:\n",
    "    trains=[]\n",
    "    tests=[]\n",
    "    losses=[]\n",
    "    acc_train=[]\n",
    "    acc_test=[]\n",
    "\n",
    "    for x in range(experiments):\n",
    "      print(\"\\r\",i,x,end='')\n",
    "      student_model = linear_model([i]).to(device)\n",
    "      criterion = dist_loss_gen(temp)\n",
    "      optimizer = torch.optim.SGD(student_model.parameters(), lr = 0.01)\n",
    "      eval_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "      history=dist_model(teacher, student_model, epochs, criterion, eval_criterion,optimizer)\n",
    "      trains.append(history[\"train\"])\n",
    "      tests.append(history[\"test\"])\n",
    "      losses.append(history[\"loss\"])\n",
    "      acc_train.append(history[\"acc_train\"])\n",
    "      acc_test.append(history[\"acc_test\"])\n",
    "\n",
    "\n",
    "    exps[i] = {\"epoch\":history[\"epoch\"],\n",
    "               \"train\":np.array(trains).mean(axis=0),\n",
    "               \"test\":np.array(tests).mean(axis=0),\n",
    "               \"loss\":np.array(losses).mean(axis=0),\n",
    "              \"acc_train\":np.array(acc_train).mean(axis=0),\n",
    "              \"acc_test\":np.array(acc_test).mean(axis=0)\n",
    "              }\n",
    "    #models[i] = student_model\n",
    "\n",
    "  plot_exp(exps)\n",
    "  return exps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model( hidden_size,input_size=784, out_size=10):\n",
    "  layers=[input_size]+hidden_size+[out_size]\n",
    "  mod_lays=[]\n",
    "  for i in range(len(layers)-2):\n",
    "    mod_lays.append(nn.Linear(layers[i], layers[i+1])) \n",
    "    mod_lays.append( torch.nn.ReLU())\n",
    "  mod_lays.append(nn.Linear(layers[-2], layers[-1]))\n",
    "    \n",
    "  return nn.Sequential(*mod_lays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-091ef0824d91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneuronas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "neuronas=[int(i) for i in np.exp2(np.arange(0,10))]\n",
    "\n",
    "epochs=50\n",
    "temp=3.5\n",
    "\n",
    "teacher=net\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "ex=distillation_experiment(neuronas, epochs, temp, teacher,experiments=1)\n",
    "p=pd.DataFrame.from_dict(ex)\n",
    "\n",
    "with open(\"expDist%f.csv\"%temp, \"w\") as text_file:\n",
    "    text_file.write(p.to_csv(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composed_loss(alpha=0.5, T=8):\n",
    "  def total_loss(student_scores, teacher_scores,y, alpha=alpha, T=T):\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(student_scores/T, dim=1),\n",
    "                             F.softmax(teacher_scores/T, dim=1))\n",
    "    \n",
    "  \n",
    "    CE_loss=nn.CrossEntropyLoss()(student_scores.squeeze(), y) \n",
    "\n",
    "    return CE_loss*alpha*temp*temp + KD_loss*(1-alpha)\n",
    "\n",
    "  return total_loss\n",
    "  \n",
    "def sample(loader):\n",
    "    data,target=next(iter(loader))\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    return data,target\n",
    "\n",
    "def dist_model(T_model, S_model, epochs, criterion, eval_criterion, optimizer):\n",
    "  global train_loader, test_loader\n",
    "  #train_loader, x_test,y_test = experiment_data()\n",
    "  history={\"epoch\":[],\n",
    "           \"train\":[],\n",
    "           \"test\":[],\n",
    "           \"loss\":[],\n",
    "           \"acc_train\":[],\n",
    "           \"acc_test\":[]\n",
    "          }\n",
    "  \n",
    "  S_model.train()\n",
    "  T_model.eval()\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):# 784= 28*28\n",
    "        x_train, y_train = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        \n",
    "        # Predecir\n",
    "        S_y_pred = S_model(x_train.view(-1,784))\n",
    "        T_y_pred = T_model(x_train)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(S_y_pred,T_y_pred)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    if epoch % 1 == 0:\n",
    "        x_train, y_train= sample(train_loader)\n",
    "        y_pred = S_model(x_train.view(-1,784))\n",
    "        train_stats = eval_criterion(y_pred, y_train) \n",
    "        \n",
    "        x_test, y_test= sample(test_loader)\n",
    "        y_pred = S_model(x_test.view(-1,784))\n",
    "        test_stats = eval_criterion(y_pred.squeeze(), y_test) \n",
    "        y_predT = T_model(x_test)\n",
    "        test_statsT = eval_criterion(y_predT.squeeze(), y_test) \n",
    "\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        history[\"test\"].append(test_stats.item())\n",
    "        history[\"train\"].append(train_stats.item())\n",
    "        history[\"acc_test\"].append(test( S_model, train_loader))\n",
    "        history[\"acc_train\"].append(test( S_model, test_loader))\n",
    "        #print('Epoch {}: train loss: {}, test loss: {}'.format(epoch, loss.item(), test_stats.item()) )\n",
    "\n",
    "  return history\n",
    "\n",
    "def distillation_experiment(neuronas, epochs, temp, teacher, experiments=5):\n",
    "  exps={}\n",
    "  dist_models={}\n",
    "  \n",
    "  for i in neuronas:\n",
    "    trains=[]\n",
    "    tests=[]\n",
    "    losses=[]\n",
    "    acc_train=[]\n",
    "    acc_test=[]\n",
    "    \n",
    "    for x in range(experiments):\n",
    "      print(\"\\r\",i,x,end='')\n",
    "      student_model = linear_model([i]).to(device)\n",
    "      criterion = dist_loss_gen(temp)\n",
    "      optimizer = torch.optim.SGD(student_model.parameters(), lr = 0.01)\n",
    "      eval_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "      history=dist_model(teacher, student_model, epochs, criterion, eval_criterion,optimizer)\n",
    "      trains.append(history[\"train\"])\n",
    "      tests.append(history[\"test\"])\n",
    "      losses.append(history[\"loss\"])\n",
    "      acc_train.append(history[\"acc_train\"])\n",
    "      acc_test.append(history[\"acc_test\"])\n",
    "\n",
    "    exps[i] = {\"epoch\":history[\"epoch\"],\n",
    "               \"train\":np.array(trains).mean(axis=0),\n",
    "               \"test\":np.array(tests).mean(axis=0),\n",
    "               \"loss\":np.array(losses).mean(axis=0),\n",
    "               \"acc_train\":np.array(acc_train).mean(axis=0),\n",
    "              \"acc_test\":np.array(acc_test).mean(axis=0)}\n",
    "    #models[i] = student_model\n",
    "\n",
    "  plot_exp(exps)\n",
    "  return exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronas=[int(i) for i in np.exp2(np.arange(0,10))]\n",
    "\n",
    "epochs=50\n",
    "temp=3.5\n",
    "\n",
    "teacher=net\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "ex=distillation_experiment(neuronas, epochs, temp, teacher,experiments=1)\n",
    "p=pd.DataFrame.from_dict(ex)\n",
    "\n",
    "with open(\"expDist%f.csv\"%temp, \"w\") as text_file:\n",
    "    text_file.write(p.to_csv(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destilacion de features, MLP 1 capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
