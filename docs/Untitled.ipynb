{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation.md         My Collection.bib       Untitled.ipynb\r\n",
      "Feature Distillation.md Teoria.md               bibtex.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybtex in /Users/jpcosec/anaconda3/lib/python3.7/site-packages (0.22.2)\n",
      "Requirement already satisfied: six in /Users/jpcosec/anaconda3/lib/python3.7/site-packages (from pybtex) (1.12.0)\n",
      "Requirement already satisfied: PyYAML>=3.01 in /Users/jpcosec/anaconda3/lib/python3.7/site-packages (from pybtex) (3.13)\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in /Users/jpcosec/anaconda3/lib/python3.7/site-packages (from pybtex) (1.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pybtex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybtex.database import parse_file\n",
    "bib_data = parse_file('My Collection.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=\"\"\"### A Comprehensive Overhaul of Feature Distillation \n",
    "### Mimicking Very Efficient Network for Object Detection\n",
    "### Structured Knowledge Distillation for Semantic Segmentation\n",
    "### Knowledge Distillation with Feature Maps for Image Classification\n",
    "### Distilling Object Detectors with Fine-grained Feature Imitation\n",
    "### An Embarrassingly Simple Approach for Knowledge Distillation\n",
    "### Learning Student Networks via Feature Embedding\n",
    "### FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification\n",
    "### Like What You Like: Knowledge Distill via Neuron Selectivity Transfer \n",
    "### FITNETS: HINTS FOR THIN DEEP NETS\n",
    "### Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\n",
    "### Paraphrasing Complex Network: Network Compression via Factor Transfer\n",
    "### Layer-Level Knowledge Distillation for Deep Neural Network Learning\n",
    "### A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning\n",
    "### Learning Deep Representations with Probabilistic Knowledge Transfer\n",
    "### Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons\n",
    "### Learning Efficient Object Detection Models with Knowledge Distillation\n",
    "### Knowledge Transfer with Jacobian Matching \n",
    "### Accelerating Convolutional Neural Networks with Dominant Convolutional Kernel and Knowledge Pre-regression\n",
    "### FEED: FEATURE-LEVEL ENSEMBLE EFFECT FOR KNOWLEDGE DISTILLATION\"\"\"\n",
    "\n",
    "srr=st.replace(\"### \",\"\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tits={}\n",
    "for entry in bib_data.entries:\n",
    "    title=bib_data.entries[entry].fields['title']\n",
    "    tits[title[1:-1].strip]=entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Overhaul of Feature Distillation\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A Comprehensive Overhaul of Feature Distillation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0ee6f623ee9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'A Comprehensive Overhaul of Feature Distillation'"
     ]
    }
   ],
   "source": [
    "for s in srr:\n",
    "    print(s.strip())\n",
    "    print(tits[s.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning': 'Gal2015',\n",
       " 'Uncertainty in Deep Learning': 'Gal2016',\n",
       " 'Relational Knowledge Distillation': 'Park2019',\n",
       " 'Back to the Future: Knowledge Distillation for Human Action Anticipation': 'Tran2019',\n",
       " 'FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification': 'Ge2018',\n",
       " 'Defensive Distillation is Not Robust to Adversarial Examples': 'Carlini',\n",
       " 'Mimicking Very Efficient Network for Object Detection': 'Li2017b',\n",
       " 'Graph Distillation for Action Detection with Privileged Modalities': 'Luo2017',\n",
       " 'Keeping Neural Networks Simple by Minimizing the Description Length of the Weights': 'Hinton',\n",
       " 'Accelerating Convolutional Neural Networks with Dominant Convolutional Kernel and Knowledge Pre-regression': 'Wang2016a',\n",
       " 'Learning Student Networks via Feature Embedding': 'Chen2018a',\n",
       " 'ThunderNet: Towards Real-time Generic Object Detection': 'Qin2019',\n",
       " 'Generative Knowledge Distillation for General Purpose Function Compression': 'Riemer',\n",
       " 'Shallowing Deep Networks: Layer-wise Pruning based on Feature Representations': 'Chen2018b',\n",
       " 'How Does Batch Normalization Help Optimization?': 'Santurkar2018',\n",
       " 'One weird trick for parallelizing convolutional neural networks': 'Krizhevsky2014',\n",
       " 'Recent Advances in Convolutional Neural Networks': 'Gu2015',\n",
       " 'Adversarial Distillation of Bayesian Neural Network Posteriors': 'Wang2018',\n",
       " 'Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks': 'Papernot2016',\n",
       " 'Paraphrasing Complex Network: Network Compression via Factor Transfer': 'Kim',\n",
       " 'Knowledge distillation using unlabeled mismatched images': 'Kulkarni2017',\n",
       " 'Learning from Multiple Teacher Networks': 'You2017',\n",
       " 'Learning to Specialize with Knowledge Distillation for Visual Question Answering': 'Mun2018',\n",
       " 'U-Net: Convolutional Networks for Biomedical Image Segmentation': 'Ronneberger',\n",
       " 'Striving for Simplicity: The All Convolutional Net': 'Springenberg2014',\n",
       " 'Adversarial Learning of Portable Student Networks': 'Wang2018b',\n",
       " 'Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement': 'Shen2019',\n",
       " 'Holistic CNN Compression via Low-rank Decomposition with Knowledge Transfer': 'Lin2018',\n",
       " 'Continual lifelong learning with neural networks: A review': 'Parisi2019',\n",
       " 'DSSD : Deconvolutional Single Shot Detector': 'Fu2017',\n",
       " 'MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models': 'Pan2018',\n",
       " 'A Cross-Modal Distillation Network for Person Re-identification in RGB-Depth': 'Hafner2018',\n",
       " 'Paying More Attention to Motion: Attention Distillation for Learning Video Representations': 'Liu2019',\n",
       " 'Pattern Recognition and Machine Learning': 'Bishop',\n",
       " 'Efficient Processing of Deep Neural Networks: A Tutorial and Survey': 'Sze2017',\n",
       " 'Distillation of a CNN for a high accuracy mobile face recognition system': 'Guzzi2019',\n",
       " 'Bayesian Dark Knowledge': 'Korattikara',\n",
       " 'Feature Partitioning for Efficient Multi-Task Architectures': 'Newell2019',\n",
       " 'Neural Processes': 'Garnelo2018',\n",
       " '3D Graph Neural Networks for RGBD Semantic Segmentation': 'Qi',\n",
       " 'A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning': 'Yim2017',\n",
       " 'Super accurate low latency object detection on a surveillance UAV': 'Vandersteegen2019',\n",
       " 'Knowledge Distillation by On-the-Fly Native Ensemble': 'Lan2018',\n",
       " 'Exploiting Parallelism Opportunities with Deep Learning Frameworks': 'Wang2019b',\n",
       " 'A Tutorial on Support Vector Machines for Pattern Recognition': 'Burges',\n",
       " 'Network In Network': 'Lin2013',\n",
       " 'A Survey of Model Compression and Acceleration for Deep Neural Networks': 'Cheng',\n",
       " 'Deep Forest': 'Zhou2017',\n",
       " 'Survey and Taxonomy of Lossless Graph Compression and Space-Efficient Graph Representations': 'Besta2018',\n",
       " 'Relation Distillation Networks for Video Object Detection': 'Deng2019',\n",
       " 'Probabilistic machine learning and artificial intelligence': 'Ghahramani2015',\n",
       " 'Generative Adversarial Nets': 'Goodfellow2014',\n",
       " 'Data-Free Learning of Student Networks': 'Chen2019a',\n",
       " 'Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy': 'Mishra2018',\n",
       " 'Self-supervised Knowledge Distillation Using Singular Value Decomposition': 'Lee2018',\n",
       " 'Image compression techniques: A survey in lossless and lossy algorithms': 'Hussain2018',\n",
       " 'GAN-Knowledge Distillation for one-stage Object Detection': 'Hong2019',\n",
       " 'Large scale distributed neural network training through online distillation': 'Anil2018',\n",
       " 'Dropout Distillation for Efficiently Estimating Model Confidence': 'Gurau',\n",
       " 'Dataset Culling: Towards Efficient Training Of Distillation-Based Domain Specific Models': 'Yoshioka2019',\n",
       " 'Knowledge Distillation for End-to-End Person Search': 'Munjal2019',\n",
       " 'Graph Distillation for Action Detection with Privileged Modalities Target Train Few Examples, A Subset of Modalities Abundant Example, Multiple Modalities': 'Luo2018',\n",
       " 'Teacher-Student Compression with Generative Adversarial Networks': 'Anonymous2019',\n",
       " 'Few-shot learning of neural networks from scratch by pseudo example optimization': 'Kimura2018',\n",
       " 'Moonshine: Distilling with Cheap Convolutions': 'Crowley',\n",
       " 'An Embarrassingly Simple Approach for Knowledge Distillation': 'Gao2018',\n",
       " 'Deep Neural Network Compression by In-Parallel Pruning-Quantization': 'Tung2018',\n",
       " 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting': 'Srivastava2014',\n",
       " 'Recent progresses on object detection: a brief review': 'Zhang2019a',\n",
       " 'DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer': 'Chen2017a',\n",
       " 'YouTube-8M: A Large-Scale Video Classification Benchmark': 'Abu-El-Haija2016',\n",
       " 'KDGAN: Knowledge Distillation with Generative Adversarial Networks': 'Wang2018a',\n",
       " 'Learning both Weights and Connections for Efficient Neural Networks': 'Han2015',\n",
       " 'Theoretical Insights Into the Optimization Landscape of Over-Parameterized Shallow Neural Networks': 'Soltanolkotabi2019',\n",
       " 'Learning efficient object detection models with knowledge distillation': 'Chen2017',\n",
       " 'Bayesian Compression for Deep Learning': 'Louizos',\n",
       " 'Fast Training of Convolutional Networks through FFTs': 'Mathieu2013',\n",
       " 'Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition': 'Lebedev2014',\n",
       " 'Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning': 'Yang2016',\n",
       " 'Domain adaptation of DNN acoustic models using knowledge distillation': 'Asami2017',\n",
       " 'Net2Net: Accelerating Learning via Knowledge Transfer': 'Chen2015',\n",
       " 'Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation': 'Zhang2019',\n",
       " 'Distill-Net: Application-Specific Distillation of Deep Convolutional Neural Networks for Resource-Constrained IoT Platforms': 'Motamedi2018',\n",
       " 'A guide to convolution arithmetic for deep learning': 'Dumoulin2016',\n",
       " 'Learning Global Additive Explanations for Neural Nets Using Model Distillation': 'Tan2018a',\n",
       " 'LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning': 'Anonymous2019d',\n",
       " 'Zero-shot Knowledge Transfer via Adversarial Belief Matching': 'Micaelli2019',\n",
       " 'Highlight Every Step : Improved Knowledge Distillation Method via Collaborative Teaching': 'Zhao2019',\n",
       " 'Structured Knowledge Distillation for Semantic Segmentation': 'Liu2019a',\n",
       " 'Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree': 'Lee2015',\n",
       " 'Computer Vision – ECCV 2016': 'Leibe2016',\n",
       " 'Dataset Distillation': 'Wang2019',\n",
       " 'KTAN: Knowledge Transfer Adversarial Network': 'Liu2018',\n",
       " 'Zero-Shot Knowledge Distillation in Deep Networks': 'Nayak2019',\n",
       " 'A Survey of Methods for Explaining Black Box Models': 'Guidotti2018',\n",
       " 'Cerema Metro Station Dataset (CEMEST) for Analyzing and Monitoring Passenger Flow in Public Transport System': 'Hieu2019',\n",
       " 'What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?': 'Kendall',\n",
       " 'Distilling Object Detectors with Fine-grained Feature Imitation': 'Wang2019a',\n",
       " 'TKD: Temporal Knowledge Distillation for Active Perception': 'Farhadi2019',\n",
       " 'Policy Distillation': 'Rusu2015',\n",
       " 'Cross Modal Distillation for Supervision Transfer': 'Gupta',\n",
       " 'Feature-map-level Online Adversarial Knowledge Distillation': 'Anonymous2019c',\n",
       " 'Deep Learning': 'Goodfellow2016',\n",
       " 'Deep Distillation Recursive Network for Remote Sensing Imagery Super-Resolution': 'Jiang2018',\n",
       " 'Understanding Deep Image Representations by Inverting Them': 'Mahendran2014',\n",
       " 'Emergence of Invariance and Disentanglement in Deep Representations': 'Achille2017',\n",
       " 'On the Connection between Neural Processes and Gaussian Processes with Deep Kernels': 'Rudner',\n",
       " 'Deep Learning for Single Image Super-Resolution: A Brief Review': 'Yang',\n",
       " 'A survey of transfer learning': 'Weiss2016',\n",
       " 'Dropout Distillation': 'Bulo2016',\n",
       " 'Learning Efficient Detector with Semi-supervised Adaptive Distillation': 'Tang2019',\n",
       " 'Regularization of neural networks using dropconnect': 'Wan2013',\n",
       " 'Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks': 'Anonymous2019a',\n",
       " 'Data-Free Knowledge Distillation for Deep Neural Networks': 'Lopes2017',\n",
       " 'DAFL: Data-Free Learning of Student Networks.': 'Chen2019',\n",
       " 'A Comprehensive Overhaul of Feature Distillation': 'Heo2019',\n",
       " 'FEED: Feature-level Ensemble Effect for knowledge Distillation': 'Park2018',\n",
       " 'Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons': 'Heo2018',\n",
       " 'Auto-Encoding Variational Bayes': 'Kingma2013',\n",
       " 'Opening the Black Box of Deep Neural Networks via Information': 'Shwartz-Ziv2017',\n",
       " 'SlimNets: An Exploration of Deep Model Compression and Acceleration': 'Oguntola2018',\n",
       " 'Unifying Heterogeneous Classifiers with Distillation': 'Vongkulbhisal2019',\n",
       " 'Variational Information Distillation for Knowledge Transfer': 'Ahn2019',\n",
       " 'The Cityscapes Dataset for Semantic Urban Scene Understanding': 'Cordts2016',\n",
       " 'All you need is a good init': 'Mishkin2015',\n",
       " 'Constructing Deep Neural Networks by Bayesian Network Structure Learning': 'Rohekar2018',\n",
       " 'Learning From Noisy Labels With Distillation': 'Li2017a',\n",
       " 'Layer-Level Knowledge Distillation for Deep Neural Network Learning': 'Li2019',\n",
       " 'Few Sample Knowledge Distillation for Efficient Network Compression': 'Li2018a',\n",
       " 'Distilling the Knowledge in a Neural Network': 'Hinton2015',\n",
       " 'Knowledge Distillation from Few Samples': 'Li2018',\n",
       " 'Data Distillation: Towards Omni-Supervised Learning': 'Radosavovic2018',\n",
       " 'Knowledge transfer with jacobian matching': 'Srinivas2018',\n",
       " 'Conditional Graph Neural Processes: A Functional Autoencoder Approach': 'Nassar',\n",
       " 'NoScope: Optimizing Neural Network Queries over Video at Scale': 'Kang2150',\n",
       " 'Fast and Accurate Single Image Super-Resolution via Information Distillation Network': 'Hui2018',\n",
       " 'Do Deep Nets Really Need to be Deep?': 'Ba2013',\n",
       " 'New trends on moving object detection in video images captured by a moving camera: A survey': 'Yazdi2018',\n",
       " 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift': 'Ioffe2015',\n",
       " 'Batch Normalization is a Cause of Adversarial Vulnerability': 'Galloway2019',\n",
       " 'Deeply-Fused Nets': 'Wang2016',\n",
       " 'Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach': 'Zhou2018a',\n",
       " 'DistInit: Learning Video Representations Without a Single Labeled Video': 'Girdhar2019',\n",
       " 'Model Compression': 'Bucil2006',\n",
       " 'AVD: Adversarial Video Distillation': 'Tavakolian2019',\n",
       " 'Improving neural networks by preventing co-adaptation of feature detectors': 'Hintona',\n",
       " 'Deep Neural Networks with Massive Learned Knowledge': 'Hu',\n",
       " 'Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks': 'Anonymous2019b',\n",
       " 'Bypass Enhancement RGB Stream Model for Pedestrian Action Recognition of Autonomous Vehicles': 'Cao2019',\n",
       " 'Efficient Knowledge Distillation from an Ensemble of Teachers': 'Fukuda2017',\n",
       " 'CondenseNet: An Efficient DenseNet Using Learned Group Convolutions': 'Huang2018',\n",
       " 'Deep Residual Learning for Image Recognition': 'He2015',\n",
       " 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks': 'Sermanet2013',\n",
       " 'Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks': 'Xu2017',\n",
       " 'Unsupervised Pre-Training of Image Features on Non-Curated Data': 'Caron2019',\n",
       " 'Knowledge Distillation with Feature Maps for Image Classification': 'Chen2018',\n",
       " 'Generalization in Deep Learning': 'Kawaguchi2017',\n",
       " 'Understanding deep learning requires rethinking generalization': 'Zhang2016',\n",
       " 'The Expressive Power of Neural Networks: A View from the Width': 'Lu',\n",
       " 'Approximation by superpositions of a sigmoidal function': 'Cybenko1989',\n",
       " 'Approximation capabilities of multilayer feedforward networks': 'Hornik1991',\n",
       " 'Like What You Like: Knowledge Distill via Neuron Selectivity Transfer': 'Huang2017',\n",
       " 'Learning Deep Representations with Probabilistic Knowledge Transfer': 'Passalis2018',\n",
       " 'Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer': 'Zagoruyko2016',\n",
       " 'Object detection at 200 Frames Per Second': 'Mehta',\n",
       " 'On the number of response regions of deep feed forward networks with piece-wise linear activations': 'Pascanu2013',\n",
       " 'How transferable are features in deep neural networks?': 'Yosinski2014',\n",
       " 'Unifying distillation and privileged information': 'Lopez-Paz2015',\n",
       " 'Exploring Generalization in Deep Learning': 'Neyshabur2017',\n",
       " 'Benefits of depth in neural networks': 'Telgarsky2016',\n",
       " 'Expressiveness of Rectifier Networks': 'Pan2015',\n",
       " 'Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations': 'Hanin2017',\n",
       " 'FitNets: Hints for Thin Deep Nets': 'Romero2014',\n",
       " 'Rademacher and Gaussian Complexities: Risk Bounds and Structural Results Peter': 'Bartlett2002',\n",
       " 'Knowledge Distillation for Small-footprint Highway Networks': 'Lu2016'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
