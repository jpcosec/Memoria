,year,url,title,author,ID,month,publisher,doi,arxivid,archiveprefix,abstract,journal
83,2013,http://arxiv.org/abs/1312.6229,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks","Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann",Sermanet2013,dec,,,1312.6229,arXiv,"We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
61,2013,http://arxiv.org/abs/1312.5851,Fast Training of Convolutional Networks through FFTs,"Mathieu, Michael and Henaff, Mikael and LeCun, Yann",Mathieu2013,dec,,,1312.5851,arXiv,"Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
85,2014,http://arxiv.org/abs/1412.6806,Striving for Simplicity: The All Convolutional Net,"Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin",Springenberg2014,dec,,,1412.6806,arXiv,"Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the ""deconvolution approach"" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
104,2014,http://arxiv.org/abs/1411.1792,How transferable are features in deep neural networks?,"Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod",Yosinski2014,nov,,,1411.1792,arXiv,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
44,2014,http://arxiv.org/abs/1412.6553,Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition,"Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor",Lebedev2014,dec,,,1412.6553,arXiv,"We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of \$1\backslash%\$ increase of the overall top-5 classification error.",
80,2014,http://arxiv.org/abs/1412.6550,FitNets: Hints for Thin Deep Nets,"Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua",Romero2014,dec,,,1412.6550,arXiv,"While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
30,2015,http://arxiv.org/abs/1512.03385,Deep Residual Learning for Image Recognition,"He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",He2015,dec,,,1512.03385,arXiv,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
64,2015,http://arxiv.org/abs/1511.06422,All you need is a good init,"Mishkin, Dmytro and Matas, Jiri",Mishkin2015,nov,,,1511.06422,arXiv,"Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.",
33,2015,http://arxiv.org/abs/1503.02531,Distilling the Knowledge in a Neural Network,"Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff",Hinton2015,,,,1503.02531,arXiv,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
45,2015,http://arxiv.org/abs/1509.08985,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree","Lee, Chen-Yu and Gallagher, Patrick W. and Tu, Zhuowen",Lee2015,sep,,,1509.08985,arXiv,"We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.",
16,2015,http://arxiv.org/abs/1511.05641,Net2Net: Accelerating Learning via Knowledge Transfer,"Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon",Chen2015,nov,,,1511.05641,arXiv,"We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.",
56,2015,http://arxiv.org/abs/1511.03643,Unifying distillation and privileged information,"Lopez-Paz, David and Bottou, Léon and Schölkopf, Bernhard and Vapnik, Vladimir",Lopez-Paz2015,nov,,,1511.03643,arXiv,"Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data.",
82,2015,http://arxiv.org/abs/1511.06295,Policy Distillation,"Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia",Rusu2015,nov,,,1511.06295,arXiv,"Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",
93,2016,http://arxiv.org/abs/1605.07716,Deeply-Fused Nets,"Wang, Jingdong and Wei, Zhen and Zhang, Ting and Zeng, Wenjun",Wang2016,may,,,1605.07716,arXiv,"In this paper, we present a novel deep learning approach, deeply-fused nets. The central idea of our approach is deep fusion, i.e., combine the intermediate representations of base networks, where the fused output serves as the input of the remaining part of each base network, and perform such combinations deeply over several intermediate representations. The resulting deeply fused net enjoys several benefits. First, it is able to learn multi-scale representations as it enjoys the benefits of more base networks, which could form the same fused network, other than the initial group of base networks. Second, in our suggested fused net formed by one deep and one shallow base networks, the flows of the information from the earlier intermediate layer of the deep base network to the output and from the input to the later intermediate layer of the deep base network are both improved. Last, the deep and shallow base networks are jointly learnt and can benefit from each other. More interestingly, the essential depth of a fused net composed from a deep base network and a shallow base network is reduced because the fused net could be composed from a less deep base network, and thus training the fused net is less difficult than training the initial deep base network. Empirical results demonstrate that our approach achieves superior performance over two closely-related methods, ResNet and Highway, and competitive performance compared to the state-of-the-arts.",
58,2016,http://arxiv.org/abs/1608.00892,Knowledge Distillation for Small-footprint Highway Networks,"Lu, Liang and Guo, Michelle and Renals, Steve",Lu2016,aug,,,1608.00892,arXiv,"Deep learning has significantly advanced state-of-the-art of speech recognition in the past few years. However, compared to conventional Gaussian mixture acoustic models, neural network models are usually much larger, and are therefore not very deployable in embedded devices. Previously, we investigated a compact highway deep neural network (HDNN) for acoustic modelling, which is a type of depth-gated feedforward neural network. We have shown that HDNN-based acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to plain deep neural network (DNN) acoustic models. In this paper, we push the boundary further by leveraging on the knowledge distillation technique that is also known as \$\backslash$it teacher-student\ training, i.e., we train the compact HDNN model with the supervision of a high accuracy cumbersome model. Furthermore, we also investigate sequence training and adaptation in the context of teacher-student training. Our experiments were performed on the AMI meeting speech recognition corpus. With this technique, we significantly improved the recognition accuracy of the HDNN acoustic model with less than 0.8 million parameters, and narrowed the gap between this model and the plain DNN with 30 million parameters.",
98,2016,http://link.springer.com/10.1007/978-3-319-46484-8_32,Accelerating Convolutional Neural Networks with Dominant Convolutional Kernel and Knowledge Pre-regression,"Wang, Zhenyang and Deng, Zhidong and Wang, Shiyao",Wang2016a,,"Springer, Cham",10.1007/978-3-319-46484-8_32,,,,
100,2016,http://arxiv.org/abs/1611.05128,Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning,"Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne",Yang2016,nov,,,1611.05128,arXiv,"Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation. To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited. Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html",
46,2016,http://link.springer.com/10.1007/978-3-319-46484-8,Computer Vision – ECCV 2016,,Leibe2016,,Springer International Publishing,10.1007/978-3-319-46484-8,,,,
81,2016,http://proceedings.mlr.press/v48/bulo16.html,Dropout Distillation,"Rota Bulò, Samuel and Porzi, Lorenzo and Kontschieder, Peter",Bulo2016,jun,,,,,"Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomiza-tion process allows to implicitly train an ensem-ble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typi-cal workaround for this intractable averaging op-eration consists in scaling the layers undergoing dropout randomization. This simple rule called "" standard dropout "" is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined "" dropout dis-tillation "" , that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping un-der control its computational efficiency. We are thus able to construct models that are as effi-cient as standard dropout, or even more efficient, while being more accurate. Experiments on stan-dard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout.",
105,2016,http://arxiv.org/abs/1612.03928,Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,"Zagoruyko, Sergey and Komodakis, Nikos",Zagoruyko2016,dec,,,1612.03928,arXiv,"Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer",
106,2016,http://arxiv.org/abs/1611.03530,Understanding deep learning requires rethinking generalization,"Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol",Zhang2016,nov,,,1611.03530,arXiv,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
71,2016,http://ieeexplore.ieee.org/document/7546524/,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,"Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram",Papernot2016,may,IEEE,10.1109/SP.2016.41,,,,
57,2017,http://arxiv.org/abs/1705.08665,Bayesian Compression for Deep Learning,"Louizos, Christos and Ullrich, Karen and Welling, Max",Louizos,,,,1705.08665,arXiv,"Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.",
42,2017,http://arxiv.org/abs/1703.07131,Knowledge distillation using unlabeled mismatched images,"Kulkarni, Mandar and Patil, Kalpesh and Karande, Shirish",Kulkarni2017,mar,,,1703.07131,arXiv,"Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.",
36,2017,http://arxiv.org/abs/1707.01219,Like What You Like: Knowledge Distill via Neuron Selectivity Transfer,"Huang, Zehao and Wang, Naiyan",Huang2017,jul,,,1707.01219,arXiv,"Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.",
60,2017,http://arxiv.org/abs/1712.00108,Graph Distillation for Action Detection with Privileged Modalities,"Luo, Zelun and Hsieh, Jun-Ting and Jiang, Lu and Niebles, Juan Carlos and Fei-Fei, Li",Luo2017,nov,,,1712.00108,arXiv,"We propose a technique that tackles action detection in multimodal videos under a realistic and challenging condition in which only limited training data and partially observed modalities are available. Common methods in transfer learning do not take advantage of the extra modalities potentially available in the source domain. On the other hand, previous work on multimodal learning only focuses on a single domain or task and does not handle the modality discrepancy between training and testing. In this work, we propose a method termed graph distillation that incorporates rich privileged information from a large-scale multimodal dataset in the source domain, and improves the learning in the target domain where training data and modalities are scarce. We evaluate our approach on action classification and detection tasks in multimodal videos, and show that our model outperforms the state-of-the-art by a large margin on the NTU RGB+D and PKU-MMD benchmarks. The code is released at http://alan.vision/eccv18_graph/.",
48,2017,http://ieeexplore.ieee.org/document/8100259/,Mimicking Very Efficient Network for Object Detection,"Li, Quanquan and Jin, Shengying and Yan, Junjie",Li2017,jul,IEEE,10.1109/CVPR.2017.776,,,,
55,2017,http://arxiv.org/abs/1710.07535,Data-Free Knowledge Distillation for Deep Neural Networks,"Lopes, Raphael Gontijo and Fenu, Stefano and Starner, Thad",Lopes2017,oct,,,1710.07535,arXiv,"Recent advances in model compression have provided procedures for compressing large neural networks to a fraction of their original size while retaining most if not all of their accuracy. However, all of these approaches rely on access to the original training set, which might not always be possible if the network to be compressed was trained on a very large dataset, or on a dataset whose release poses privacy or safety concerns as may be the case for biometrics tasks. We present a method for data-free knowledge distillation, which is able to compress deep neural networks trained on large-scale datasets to a fraction of their size leveraging only some extra metadata to be provided with a pretrained model release. We also explore different kinds of metadata that can be used with our method, and discuss tradeoffs involved in using each of them.",
51,2017,http://openaccess.thecvf.com/content_iccv_2017/html/Li_Learning_From_Noisy_ICCV_2017_paper.html,Learning From Noisy Labels With Distillation,"Li, Yuncheng and Yang, Jianchao and Song, Yale and Cao, Liangliang and Luo, Jiebo and Li, Li-Jia",Li2017a,,,,,,,
21,2017,http://dx.doi.org/10.21437/Interspeech.2017-614,Efficient Knowledge Distillation from an Ensemble of Teachers,"Fukuda, Takashi and Suzuki, Masayuki and Kurata, Gakuto and Thomas, Samuel and Cui, Jia and Ramabhadran, Bhuvana",Fukuda2017,,,10.21437/Interspeech.2017-614,,,"This paper describes the effectiveness of knowledge distillation using teacher student training for building accurate and compact neural networks. We show that with knowledge distillation , information from multiple acoustic models like very deep VGG networks and Long Short-Term Memory (LSTM) models can be used to train standard convolutional neural network (CNN) acoustic models for a variety of systems requiring a quick turnaround. We examine two strategies to leverage multiple teacher labels for training student models. In the first technique , the weights of the student model are updated by switching teacher labels at the minibatch level. In the second method, student models are trained on multiple streams of information from various teacher distributions via data augmentation. We show that standard CNN acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to teacher VGG and LSTM acoustic models. Additionally we also investigate the effectiveness of using broadband teacher labels as privileged knowledge for training better narrowband acoustic models within this framework. We show the benefit of this simple technique by training narrow-band student models with broadband teacher soft labels on the Aurora 4 task.",
12,2017,https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation,Learning Efficient Object Detection Models with Knowledge Distillation,"Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan",Chen2017,,,,,,,
99,2017,http://arxiv.org/abs/1709.00513,Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks,"Xu, Zheng and Hsu, Yen-Chang and Huang, Jiawei",Xu2017,sep,,,1709.00513,arXiv,"There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study the trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student network.",
8,2017,http://arxiv.org/abs/1709.07604,"A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications","Cai, Hongyun and Zheng, Vincent W. and Chang, Kevin Chen-Chuan",Cai2017,,,10.1109/TKDE.2018.2807452,1709.07604,arXiv,"Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.",
7,2017,http://ieeexplore.ieee.org/document/7953145/,Domain adaptation of DNN acoustic models using knowledge distillation,"Asami, Taichi and Masumura, Ryo and Yamaguchi, Yoshikazu and Masataki, Hirokazu and Aono, Yushi",Asami2017,mar,IEEE,10.1109/ICASSP.2017.7953145,,,,
102,2017,http://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html,"A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning","Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo",Yim2017,,,,,,,
59,2018,http://alan.vision/eccv18,"Graph Distillation for Action Detection with Privileged Modalities Target Train Few Examples, A Subset of Modalities Abundant Example, Multiple Modalities","Luo, Zelun and Hsieh, Jun-Ting and Jiang, Lu and Niebles, Juan Carlos and Fei-Fei, Li",Luo2018,,,,1712.00108v2,arXiv,"Source Fig. 1. Our problem statement. In the source domain, we have abundant data from multiple modalities. In the target domain, we have limited data and a subset of the modalities during training, and only one modality during testing. The curved connectors between modalities represent our proposed graph distillation. Abstract. We propose a technique that tackles action detection in mul-timodal videos under a realistic and challenging condition in which only limited training data and partially observed modalities are available. Common methods in transfer learning do not take advantage of the extra modalities potentially available in the source domain. On the other hand, previous work on multimodal learning only focuses on a single domain or task and does not handle the modality discrepancy between training and testing. In this work, we propose a method termed graph distillation that incorporates rich privileged information from a large-scale multi-modal dataset in the source domain, and improves the learning in the target domain where training data and modalities are scarce. We evaluate our approach on action classification and detection tasks in multimodal videos, and show that our model outperforms the state-of-the-art by a large margin on the NTU RGB+D and PKU-MMD benchmarks. The code is released at http://alan.vision/eccv18 graph/.",
66,2018,http://arxiv.org/abs/1812.07390,Distill-Net: Application-Specific Distillation of Deep Convolutional Neural Networks for Resource-Constrained IoT Platforms,"Motamedi, Mohammad and Portillo, Felix and Fong, Daniel and Ghiasi, Soheil",Motamedi2018,dec,,,1812.07390,arXiv,"Many Internet-of-Things (IoT) applications demand fast and accurate understanding of a few key events in their surrounding environment. Deep Convolutional Neural Networks (CNNs) have emerged as an effective approach to understand speech, images, and similar high dimensional data types. Algorithmic performance of modern CNNs, however, fundamentally relies on learning class-agnostic hierarchical features that only exist in comprehensive training datasets with many classes. As a result, fast inference using CNNs trained on such datasets is prohibitive for most resource-constrained IoT platforms. To bridge this gap, we present a principled and practical methodology for distilling a complex modern CNN that is trained to effectively recognize many different classes of input data into an application-dependent essential core that not only recognizes the few classes of interest to the application accurately, but also runs efficiently on platforms with limited resources. Experimental results confirm that our approach strikes a favorable balance between classification accuracy (application constraint), inference efficiency (platform constraint), and productive development of new applications (business constraint).",
67,2018,https://papers.nips.cc/paper/8031-learning-to-specialize-with-knowledge-distillation-for-visual-question-answering,Learning to Specialize with Knowledge Distillation for Visual Question Answering,"Mun, Jonghwan and Lee, Kimin and Shin, Jinwoo and Han, Bohyung",Mun2018,,,,,,,
70,2018,https://papers.nips.cc/paper/7848-macnet-transferring-knowledge-from-machine-comprehension-to-sequence-to-sequence-models,MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,"Pan, Boyuan and Yang, Yazheng and Li, Hao and Zhao, Zhou and Zhuang, Yueting and Cai, Deng and He, Xiaofei",Pan2018,,,,,,,
73,2018,https://openreview.net/forum?id=BJxYEsAqY7,FEED: Feature-level Ensemble Effect for knowledge Distillation,"Park, SeongUk and Kwak, Nojun",Park2018,sep,,,,,,
86,2018,http://proceedings.mlr.press/v80/srinivas18a.html,Knowledge transfer with jacobian matching,"Srinivas, Suraj and Fleuret, François",Srinivas2018,jul,,,,,"Classical distillation methods transfer representations from a ""teacher"" neural network to a ""student"" network by matching their output activations. Recent methods also match the Jacobians, or the gradient of output activations with the input. However, this involves making some ad hoc decisions, in particular, the choice of the loss function. In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching. We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation. We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning.",
87,2018,http://arxiv.org/abs/1801.08640,Learning Global Additive Explanations for Neural Nets Using Model Distillation,"Tan, Sarah and Caruana, Rich and Hooker, Giles and Koch, Paul and Gordo, Albert",Tan2018a,jan,,,1801.08640,arXiv,"Interpretability has largely focused on local explanations, i.e. explaining why a model made a particular prediction for a sample. These explanations are appealing due to their simplicity and local fidelity. However, they do not provide information about the general behavior of the model. We propose to leverage model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations take the form of feature shapes, which are more expressive than feature attributions. Through careful experimentation, we show qualitatively and quantitatively that global additive explanations are able to describe model behavior and yield insights about models such as neural nets. A visualization of our approach applied to a neural net as it is trained is available at https://youtu.be/ErQYwNqzEdc.",
75,2018,http://openaccess.thecvf.com/content_ECCV_2018/html/Nikolaos_Passalis_Learning_Deep_Representations_ECCV_2018_paper.html,Learning Deep Representations with Probabilistic Knowledge Transfer,"Passalis, Nikolaos and Tefas, Anastasios",Passalis2018,,,,,,,
53,2018,http://arxiv.org/abs/1810.08126,KTAN: Knowledge Transfer Adversarial Network,"Liu, Peiye and Liu, Wu and Ma, Huadong and Mei, Tao and Seok, Mingoo",Liu2018,oct,,,1810.08126,arXiv,"To reduce the large computation and storage cost of a deep convolutional neural network, the knowledge distillation based methods have pioneered to transfer the generalization ability of a large (teacher) deep network to a light-weight (student) network. However, these methods mostly focus on transferring the probability distribution of the softmax layer in a teacher network and thus neglect the intermediate representations. In this paper, we propose a knowledge transfer adversarial network to better train a student network. Our technique holistically considers both intermediate representations and probability distributions of a teacher network. To transfer the knowledge of intermediate representations, we set high-level teacher feature maps as a target, toward which the student feature maps are trained. Specifically, we arrange a Teacher-to-Student layer for enabling our framework suitable for various student structures. The intermediate representation helps the student network better understand the transferred generalization as compared to the probability distribution only. Furthermore, we infuse an adversarial learning process by employing a discriminator network, which can fully exploit the spatial correlation of feature maps in training a student network. The experimental results demonstrate that the proposed method can significantly improve the performance of a student network on both image classification and object detection tasks.",
65,2018,https://openreview.net/forum?id=B1ae1lZRb,Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,"Mishra, Asit and Marr, Debbie",Mishra2018,feb,,,,,,
40,2018,https://www.semanticscholar.org/paper/Few-shot-learning-of-neural-networks-from-scratch-Kimura-Ghahramani/ebc50c378d25e780d5adb7e54a1c8c153761097b,Few-shot learning of neural networks from scratch by pseudo example optimization,"Kimura, Akisato and Ghahramani, Zoubin and Takeuchi, Koh and Iwata, Tomoharu and Ueda, Naonori",Kimura2018,,,,,,,undefined
23,2018,http://arxiv.org/abs/1812.01819,An Embarrassingly Simple Approach for Knowledge Distillation,"Gao, Mengya and Shen, Yujun and Li, Quanquan and Yan, Junjie and Wan, Liang and Lin, Dahua and Loy, Chen Change and Tang, Xiaoou",Gao2018,dec,,,1812.01819,arXiv,"Knowledge Distillation (KD) aims at improving the performance of a low-capacity student model by inheriting knowledge from a high-capacity teacher model. Previous KD methods typically train a student by minimizing a task-related loss and the KD loss simultaneously, using a pre-defined loss weight to balance these two terms. In this work, we propose to first transfer the backbone knowledge from a teacher to the student, and then only learn the task-head of the student network. Such a decomposition of the training process circumvents the need of choosing an appropriate loss weight, which is often difficult in practice, and thus makes it easier to apply to different datasets and tasks. Importantly, the decomposition permits the core of our method, Stage-by-Stage Knowledge Distillation (SSKD), which facilitates progressive feature mimicking from teacher to student. Extensive experiments on CIFAR-100 and ImageNet suggest that SSKD significantly narrows down the performance gap between student and teacher, outperforming state-of-the-art approaches. We also demonstrate the generalization ability of SSKD on other challenging benchmarks, including face recognition on IJB-A dataset as well as object detection on COCO dataset.",
1,2018,https://openreview.net/forum?id=rkr1UDeC-,Large scale distributed neural network training through online distillation,"Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E. and Hinton, Geoffrey E.",Anil2018,feb,,,,,,
101,2018,https://www.sciencedirect.com/science/article/pii/S1574013716301794,New trends on moving object detection in video images captured by a moving camera: A survey,"Yazdi, Mehran and Bouwmans, Thierry",Yazdi2018,may,Elsevier,10.1016/J.COSREV.2018.03.001,,,"This paper presents a survey on the latest methods of moving object detection in video sequences captured by a moving camera. Although many researches and excellent works have reviewed the methods of object detection and background subtraction for a fixed camera, there is no survey which presents a complete review of the existing different methods in the case of moving camera. Most methods in this field can be classified into four categories; modeling based background subtraction, trajectory classification, low rank and sparse matrix decomposition, and object tracking. We discuss in details each category and present the main methods which proposed improvements in the general concept of the techniques. We also present challenges and main concerns in this field as well as performance metrics and some benchmark databases available to evaluate the performance of different moving object detection algorithms.",Computer Science Review
14,2018,http://arxiv.org/abs/1812.06597,Learning Student Networks via Feature Embedding,"Chen, Hanting and Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng",Chen2018a,dec,,,1812.06597,arXiv,"Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student based methods used to rely on additional fully-connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this paper aims to propagate information from teacher to student without introducing new variables which need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features which could inherit intrinsic properties of their corresponding high-dimensional features from teacher network. The resulting portable network thus can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark datasets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.",
97,2018,http://papers.nips.cc/paper/7358-kdgan-knowledge-distillation-with-generative-adversarial-networks,KDGAN: Knowledge Distillation with Generative Adversarial Networks,"Wang, Xiaojie and Zhang, Rui and Sun, Yu and Qi, Jianzhong",Wang2018a,,,,,,,
17,2018,http://arxiv.org/abs/1812.00660,Knowledge Distillation with Feature Maps for Image Classification,"Chen, Wei-Chun and Chang, Chia-Che and Lu, Chien-Yu and Lee, Che-Rung",Chen2018,dec,,,1812.00660,arXiv,"The model reduction problem that eases the computation costs and latency of complex deep learning architectures has received an increasing number of investigations owing to its importance in model deployment. One promising method is knowledge distillation (KD), which creates a fast-to-execute student model to mimic a large teacher network. In this paper, we propose a method, called KDFM (Knowledge Distillation with Feature Maps), which improves the effectiveness of KD by learning the feature maps from the teacher network. Two major techniques used in KDFM are shared classifier and generative adversarial network. Experimental results show that KDFM can use a four layers CNN to mimic DenseNet-40 and use MobileNet to mimic DenseNet-100. Both student networks have less than 1$\backslash$% accuracy loss comparing to their teacher models for CIFAR-100 datasets. The student networks are 2-6 times faster than their teacher models for inference, and the model size of MobileNet is less than half of DenseNet-100's.",
94,2018,https://arxiv.org/pdf/1806.10317.pdf,Adversarial Distillation of Bayesian Neural Network Posteriors,"Wang, Kuan-Chieh and Vicol, Paul and Lucas, James and Gu, Li and Grosse, Roger and Zemel, Richard",Wang2018,,,,1806.10317v1,arXiv,"Bayesian neural networks (BNNs) allow us to reason about uncertainty in a principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient BNN learning by drawing samples from the BNN posterior using mini-batches. However, SGLD and its extensions require storage of many copies of the model parameters, a potentially prohibitive cost, especially for large neural networks. We propose a framework, Adversarial Posterior Distillation, to distill the SGLD samples using a Generative Adversarial Network (GAN). At test-time, samples are generated by the GAN. We show that this distillation framework incurs no loss in performance on recent BNN applications including anomaly detection , active learning, and defense against adver-sarial attacks. By construction, our framework distills not only the Bayesian predictive distribution , but the posterior itself. This allows one to compute quantities such as the approximate model variance, which is useful in downstream tasks. To our knowledge, these are the first results applying MCMC-based BNNs to the afore-mentioned applications.",
50,2018,https://www.semanticscholar.org/paper/Few-Sample-Knowledge-Distillation-for-Efficient-Li-Li/37df8ac712c44c2e0e3ab4c14c395e34f3cba733,Few Sample Knowledge Distillation for Efficient Network Compression,"Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui",Li2018a,,,,,,,
24,2018,http://arxiv.org/abs/1810.02936,FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,"Ge, Yixiao and Li, Zhuowan and Zhao, Haiyu and Yin, Guojun and Yi, Shuai and Wang, Xiaogang and Li, Hongsheng",Ge2018,oct,,,1810.02936,arXiv,"Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.",
78,2018,https://ieeexplore.ieee.org/document/8578531/,Data Distillation: Towards Omni-Supervised Learning,"Radosavovic, Ilija and Dollar, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming",Radosavovic2018,jun,IEEE,10.1109/CVPR.2018.00433,,,,
32,2018,http://arxiv.org/abs/1811.03233,Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons,"Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young",Heo2018,nov,,,1811.03233,arXiv,"An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classification friendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art.",
37,2018,http://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Fast_and_Accurate_CVPR_2018_paper.html,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,"Hui, Zheng and Wang, Xiumei and Gao, Xinbo",Hui2018,,,,,,,
38,2018,http://www.mdpi.com/2072-4292/10/11/1700,Deep Distillation Recursive Network for Remote Sensing Imagery Super-Resolution,"Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Jiang, Junjun and Xiao, Jing and Yao, Yuan and Jiang, Kui and Wang, Zhongyuan and Yi, Peng and Jiang, Junjun and Xiao, Jing and Yao, Yuan",Jiang2018,oct,Multidisciplinary Digital Publishing Institute,10.3390/rs10111700,,,"\textlessp\textgreaterDeep convolutional neural networks (CNNs) have been widely used and achieved state-of-the-art performance in many image or video processing and analysis tasks. In particular, for image super-resolution (SR) processing, previous CNN-based methods have led to significant improvements, when compared with shallow learning-based methods. However, previous CNN-based algorithms with simple direct or skip connections are of poor performance when applied to remote sensing satellite images SR. In this study, a simple but effective CNN framework, namely deep distillation recursive network (DDRN), is presented for video satellite image SR. DDRN includes a group of ultra-dense residual blocks (UDB), a multi-scale purification unit (MSPU), and a reconstruction module. In particular, through the addition of rich interactive links in and between multiple-path units in each UDB, features extracted from multiple parallel convolution layers can be shared effectively. Compared with classical dense-connection-based models, DDRN possesses the following main properties. (1) DDRN contains more linking nodes with the same convolution layers. (2) A distillation and compensation mechanism, which performs feature distillation and compensation in different stages of the network, is also constructed. In particular, the high-frequency components lost during information propagation can be compensated in MSPU. (3) The final SR image can benefit from the feature maps extracted from UDB and the compensated components obtained from MSPU. Experiments on Kaggle Open Source Dataset and Jilin-1 video satellite images illustrate that DDRN outperforms the conventional CNN-based baselines and some state-of-the-art feature extraction approaches.\textless/p\textgreater",Remote Sensing
43,2018,https://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble,Knowledge Distillation by On-the-Fly Native Ensemble,"xu Lan and Zhu, Xiatian and Gong, Shaogang",Lan2018,,,,,,,
49,2018,http://arxiv.org/abs/1812.01839,Knowledge Distillation from Few Samples,"Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui",Li2018,,,,1812.01839,arXiv,"Current knowledge distillation methods require full training data to distill knowledge from a large ""teacher"" network to a compact ""student"" network by matching certain statistics between ""teacher"" and ""student"" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both ""teacher"" and ""student"" have the same feature map sizes at each corresponding block, we add a 1x1 conv-layer at the end of each block in the student-net, and align the block-level outputs between ""teacher"" and ""student"" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed/merged into the previous conv-layer to formulate a new conv-layer with the same size of parameters and computation cost as the previous one. Experiments verify that the proposed method is very efficient and effective to distill knowledge from teacher-net to student-net constructing in different ways on various datasets.",undefined
29,2018,http://arxiv.org/abs/1810.11641,A Cross-Modal Distillation Network for Person Re-identification in RGB-Depth,"Hafner, Frank and Bhuiyan, Amran and Kooij, Julian F. P. and Granger, Eric",Hafner2018,oct,,,1810.11641,arXiv,"Person re-identification involves the recognition over time of individuals captured using multiple distributed sensors. With the advent of powerful deep learning methods able to learn discriminant representations for visual recognition, cross-modal person re-identification based on different sensor modalities has become viable in many challenging applications in, e.g., autonomous driving, robotics and video surveillance. Although some methods have been proposed for re-identification between infrared and RGB images, few address depth and RGB images. In addition to the challenges for each modality associated with occlusion, clutter, misalignment, and variations in pose and illumination, there is a considerable shift across modalities since data from RGB and depth images are heterogeneous. In this paper, a new cross-modal distillation network is proposed for robust person re-identification between RGB and depth sensors. Using a two-step optimization process, the proposed method transfers supervision between modalities such that similar structural features are extracted from both RGB and depth modalities, yielding a discriminative mapping to a common feature space. Our experiments investigate the influence of the dimensionality of the embedding space, compares transfer learning from depth to RGB and vice versa, and compares against other state-of-the-art cross-modal re-identification methods. Results obtained with BIWI and RobotPKU datasets indicate that the proposed method can successfully transfer descriptive structural features from the depth modality to the RGB modality. It can significantly outperform state-of-the-art conventional methods and deep neural networks for cross-modal sensing between RGB and depth, with no impact on computational complexity.",
92,2019,https://www.semanticscholar.org/paper/Unifying-Heterogeneous-Classifiers-with-Vongkulbhisal-Vinayavekhin/eaff90799e71e3f1a03c4ace4af8864ada9b2693 http://arxiv.org/abs/1904.06062,Unifying Heterogeneous Classifiers with Distillation,"Vongkulbhisal, Jayakorn and Vinayavekhin, Phongtharin and Visentini-Scarzanella, Marco",Vongkulbhisal2019,,,,1904.06062,arXiv,"In this paper, we study the problem of unifying knowledge from a set of classifiers with different architectures and target classes into a single classifier, given only a generic set of unlabelled data. We call this problem Unifying Heterogeneous Classifiers (UHC). This problem is motivated by scenarios where data is collected from multiple sources, but the sources cannot share their data, e.g., due to privacy concerns, and only privately trained models can be shared. In addition, each source may not be able to gather data to train all classes due to data availability at each source, and may not be able to train the same classification model due to different computational resources. To tackle this problem, we propose a generalisation of knowledge distillation to merge HCs. We derive a probabilistic relation between the outputs of HCs and the probability over all classes. Based on this relation, we propose two classes of methods based on cross-entropy minimisation and matrix factorisation, which allow us to estimate soft labels over all classes from unlabelled samples and use them in lieu of ground truth labels to train a unified classifier. Our extensive experiments on ImageNet, LSUN, and Places365 datasets show that our approaches significantly outperform a naive extension of distillation and can achieve almost the same accuracy as classifiers that are trained in a centralised, supervised manner.",undefined
91,2019,https://ieeexplore.ieee.org/document/8758060/ http://arxiv.org/abs/1904.02024,Super accurate low latency object detection on a surveillance UAV,"Vandersteegen, Maarten and Vanbeeck, Kristof and Goedeme, Toon",Vandersteegen2019,may,IEEE,10.23919/MVA.2019.8758060,1904.02024,arXiv,"Drones have proven to be useful in many industry segments such as security and surveillance, where e.g. on-board real-time object tracking is a necessity for autonomous flying guards. Tracking and following suspicious objects is therefore required in real-time on limited hardware. With an object detector in the loop, low latency becomes extremely important. In this paper, we propose a solution to make object detection for UAVs both fast and super accurate. We propose a multi-dataset learning strategy yielding top eye-sky object detection accuracy. Our model generalizes well on unseen data and can cope with different flying heights, optically zoomed-in shots and different viewing angles. We apply optimization steps such that we achieve minimal latency on embedded on-board hardware by fusing layers, quantizing calculations to 16-bit floats and 8-bit integers, with negligible loss in accuracy. We validate on NVIDIA's Jetson TX2 and Jetson Xavier platforms where we achieve a speed-wise performance boost of more than 10x.",2019 16th International Conference on Machine Vision Applications (MVA)
90,2019,http://arxiv.org/abs/1904.04868,Back to the Future: Knowledge Distillation for Human Action Anticipation,"Tran, Vinh and Wang, Yang and Hoai, Minh",Tran2019,apr,,,1904.04868,arXiv,"We consider the task of training a neural network to anticipate human actions in video. This task is challenging given the complexity of video data, the stochastic nature of the future, and the limited amount of annotated training data. In this paper, we propose a novel knowledge distillation framework that uses an action recognition network to supervise the training of an action anticipation network, guiding the latter to attend to the relevant information needed for correctly anticipating the future actions. This framework is possible thanks to a novel loss function to account for positional shifts of semantic concepts in a dynamic video. The knowledge distillation framework is a form of self-supervised learning, and it takes advantage of unlabeled data. Experimental results on JHMDB and EPIC-KITCHENS dataset show the effectiveness of our approach.",
95,2019,http://arxiv.org/abs/1906.03609,Distilling Object Detectors with Fine-grained Feature Imitation,"Wang, Tao and Yuan, Li and Zhang, Xiaopeng and Feng, Jiashi",Wang2019a,jun,,,1906.03609,arXiv,"State-of-the-art CNN based recognition models are often computationally prohibitive to deploy on low-end devices. A promising high level approach tackling this limitation is knowledge distillation, which let small student model mimic cumbersome teacher model's output to get improved generalization. However, related methods mainly focus on simple task of classification while do not consider complex tasks like object detection. We show applying the vanilla knowledge distillation to detection model gets minor gain. To address the challenge of distilling knowledge in detection model, we propose a fine-grained feature imitation method exploiting the cross-location discrepancy of feature response. Our intuition is that detectors care more about local near object regions. Thus the discrepancy of feature response on the near object anchor locations reveals important information of how teacher model tends to generalize. We design a novel mechanism to estimate those locations and let student model imitate the teacher on them to get enhanced performance. We first validate the idea on a developed lightweight toy detector which carries simplest notion of current state-of-the-art anchor based detection models on challenging KITTI dataset, our method generates up to 15% boost of mAP for the student model compared to the non-imitated counterpart. We then extensively evaluate the method with Faster R-CNN model under various scenarios with common object detection benchmark of Pascal VOC and COCO, imitation alleviates up to 74% performance drop of student model compared to teacher. Codes released at https://github.com/twangnh/Distilling-Object-Detectors",
84,2019,http://arxiv.org/abs/1908.08520,Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement,"Shen, Zhiqiang and He, Zhankui and Cui, Wanyun and Yu, Jiahui and Zheng, Yutong and Zhu, Chenchen and Savvides, Marios",Shen2019,aug,,,1908.08520,arXiv,"Generic Image recognition is a fundamental and fairly important visual problem in computer vision. One of the major challenges of this task lies in the fact that single image usually has multiple objects inside while the labels are still one-hot, another one is noisy and sometimes missing labels when annotated by humans. In this paper, we focus on tackling these challenges accompanying with two different image recognition problems: multi-model ensemble and noisy data recognition with a unified framework. As is well-known, usually the best performing deep neural models are ensembles of multiple base-level networks, as it can mitigate the variation or noise containing in the dataset. Unfortunately, the space required to store these many networks, and the time required to execute them at runtime, prohibit their use in applications where test sets are large (e.g., ImageNet). In this paper, we present a method for compressing large, complex trained ensembles into a single network, where the knowledge from a variety of trained deep neural networks (DNNs) is distilled and transferred to a single DNN. In order to distill diverse knowledge from different trained (teacher) models, we propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models, and to promote the discriminator network to distinguish teacher vs. student features simultaneously. Extensive experiments on CIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms the original model by 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a remarkable improvement of top-3 1.15% (official evaluation metric) on a strong baseline model of ResNet-101.",
96,2019,https://www.semanticscholar.org/paper/Dataset-Distillation-Wang-Zhu/07b1a0ed6ba8a497355ac105e9110a927e3cf913,Dataset Distillation,"Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.",Wang2019,,,,,,,undefined
89,2019,http://arxiv.org/abs/1907.05640,AVD: Adversarial Video Distillation,"Tavakolian, Mohammad and Sabokrou, Mohammad and Hadid, Abdenour",Tavakolian2019,jul,,,1907.05640,arXiv,"In this paper, we present a simple yet efficient approach for video representation, called Adversarial Video Distillation (AVD). The key idea is to represent videos by compressing them in the form of realistic images, which can be used in a variety of video-based scene analysis applications. Representing a video as a single image enables us to address the problem of video analysis by image analysis techniques. To this end, we exploit a 3D convolutional encoder-decoder network to encode the input video as an image by minimizing the reconstruction error. Furthermore, weak supervision by an adversarial training procedure is imposed on the output of the encoder to generate semantically realistic images. The encoder learns to extract semantically meaningful representations from a given input video by mapping the 3D input into a 2D latent representation. The obtained representation can be simply used as the input of deep models pre-trained on images for video classification. We evaluated the effectiveness of our proposed method for video-based activity recognition on three standard and challenging benchmark datasets, i.e. UCF101, HMDB51, and Kinetics. The experimental results demonstrate that AVD achieves interesting performance, outperforming the state-of-the-art methods for video classification.",
103,2019,https://www.semanticscholar.org/paper/Dataset-Culling%3A-Towards-Efficient-Training-Of-Yoshioka-Lee/dfd55d4c8fd41ad549f93b555a4d7a4c3b88f99d,Dataset Culling: Towards Efficient Training Of Distillation-Based Domain Specific Models,"Yoshioka, Kentaro and Lee, Edward and Wong, Simon and Horowitz, Mark",Yoshioka2019,,,,,,,undefined
88,2019,https://www.semanticscholar.org/paper/Learning-Efficient-Detector-with-Semi-supervised-Tang-Feng/83bc2126b808fdc5abc09d4479deea3cb7517d85,Learning Efficient Detector with Semi-supervised Adaptive Distillation,"Tang, Shitao and Feng, Litong and Shao, Wenqi and Kuang, Zhanghui and Zhang, Wenjun and Chen, Yimin",Tang2019,,,,,,,undefined
0,2019,http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html,Variational Information Distillation for Knowledge Transfer,"Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D. and Dai, Zhenwen",Ahn2019,,,,,,,
74,2019,https://www.semanticscholar.org/paper/Relational-Knowledge-Distillation-Park-Kim/0f736d2067ee9c950b876f14521268c6009e67d6 http://arxiv.org/abs/1904.05068,Relational Knowledge Distillation,"Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu",Park2019,,,,1904.05068,arXiv,"Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.",undefined
2,2019,https://openreview.net/forum?id=S1xipR4FPB,Teacher-Student Compression with Generative Adversarial Networks,Anonymous,Anonymous2019,sep,,,,,,
3,2019,https://openreview.net/forum?id=Byg_vREtvB,Generalized Bayesian Posterior Expectation Distillation for Deep Neural Networks,Anonymous,Anonymous2019a,sep,,,,,,
4,2019,https://openreview.net/forum?id=r1xGnA4Kvr,Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks,Anonymous,Anonymous2019b,sep,,,,,,
5,2019,https://openreview.net/forum?id=Bkl086VYvH,Feature-map-level Online Adversarial Knowledge Distillation,Anonymous,Anonymous2019c,sep,,,,,,
6,2019,https://openreview.net/forum?id=B1xv9pEKDS,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,Anonymous,Anonymous2019d,sep,,,,,,
9,2019,http://arxiv.org/abs/1908.05674,Bypass Enhancement RGB Stream Model for Pedestrian Action Recognition of Autonomous Vehicles,"Cao, Dong and Xu, Lisha",Cao2019,aug,,,1908.05674,arXiv,"Pedestrian action recognition and intention prediction is one of the core issues in the field of autonomous driving. In this research field, action recognition is one of the key technologies. A large number of scholars have done a lot of work to im-prove the accuracy of the algorithm for the task. However, there are relatively few studies and improvements in the computational complexity of algorithms and sys-tem real-time. In the autonomous driving application scenario, the real-time per-formance and ultra-low latency of the algorithm are extremely important evalua-tion indicators, which are directly related to the availability and safety of the au-tonomous driving system. To this end, we construct a bypass enhanced RGB flow model, which combines the previous two-branch algorithm to extract RGB feature information and optical flow feature information respectively. In the train-ing phase, the two branches are merged by distillation method, and the bypass enhancement is combined in the inference phase to ensure accuracy. The real-time behavior of the behavior recognition algorithm is significantly improved on the premise that the accuracy does not decrease. Experiments confirm the superiority and effectiveness of our algorithm.",
10,2019,http://arxiv.org/abs/1905.01278,Unsupervised Pre-Training of Image Features on Non-Curated Data,"Caron, Mathilde and Bojanowski, Piotr and Mairal, Julien and Joulin, Armand",Caron2019,may,,,1905.01278,arXiv,"Pre-training general-purpose visual features with convolutional neural networks without relying on annotations is a challenging and important task. Most recent efforts in unsupervised feature learning have focused on either small or highly curated datasets like ImageNet, whereas using uncurated raw datasets was found to decrease the feature quality when evaluated on a transfer task. Our goal is to bridge the performance gap between unsupervised methods trained on curated data, which are costly to obtain, and massive raw datasets that are easily available. To that effect, we propose a new unsupervised approach which leverages self-supervision and clustering to capture complementary statistics from large-scale data. We validate our approach on 96 million images from YFCC100M, achieving state-of-the-art results among unsupervised methods on standard benchmarks, which confirms the potential of unsupervised learning when only uncurated data are available. We also show that pre-training a supervised VGG-16 with our method achieves 74.9% top-1 classification accuracy on the validation set of ImageNet, which is an improvement of +0.8% over the same network trained from scratch. Our code is available at https://github.com/facebookresearch/DeeperCluster.",
11,2019,https://www.semanticscholar.org/paper/Knowledge-Squeezed-Adversarial-Network-Compression.-Changyong-Peng/515844aeb6705cbbc0cc899de66a631abf4b8311,Knowledge Squeezed Adversarial Network Compression.,"Changyong, Shu and Peng, Li and Yuan, Xie and Yanyun, Qu and Longquan, Dai and Lizhuang, Ma",Changyong2019,,,,,,,
13,2019,https://www.semanticscholar.org/paper/DAFL%3A-Data-Free-Learning-of-Student-Networks.-Chen-Wang/947928b4e7ff7ebb4a65d88d9c553a1fe5da7070,DAFL: Data-Free Learning of Student Networks.,"Chen, Hanting and Wang, Yunhe and Xu, C and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi Lei",Chen2019,,,,,,,
15,2019,https://www.semanticscholar.org/paper/Data-Free-Learning-of-Student-Networks-Chen-Wang/35220b60385d39f3871f838fa5618fa08ebd906d,Data-Free Learning of Student Networks,"Chen, Hanting and Wang, Yunhe and Xu, Chang and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi",Chen2019a,,,,,,,undefined
19,2019,http://arxiv.org/abs/1908.09511,Relation Distillation Networks for Video Object Detection,"Deng, Jiajun and Pan, Yingwei and Yao, Ting and Zhou, Wengang and Li, Houqiang and Mei, Tao",Deng2019,aug,,,1908.09511,arXiv,"It has been well recognized that modeling object-to-object relations would be helpful for object detection. Nevertheless, the problem is not trivial especially when exploring the interactions between objects to boost video object detectors. The difficulty originates from the aspect that reliable object relations in a video should depend on not only the objects in the present frame but also all the supportive objects extracted over a long range span of the video. In this paper, we introduce a new design to capture the interactions across the objects in spatio-temporal context. Specifically, we present Relation Distillation Networks (RDN) --- a new architecture that novelly aggregates and propagates object relation to augment object features for detection. Technically, object proposals are first generated via Region Proposal Networks (RPN). RDN then, on one hand, models object relation via multi-stage reasoning, and on the other, progressively distills relation through refining supportive object proposals with high objectness scores in a cascaded manner. The learnt relation verifies the efficacy on both improving object detection in each frame and box linking across frames. Extensive experiments are conducted on ImageNet VID dataset, and superior results are reported when comparing to state-of-the-art methods. More remarkably, our RDN achieves 81.8% and 83.2% mAP with ResNet-101 and ResNeXt-101, respectively. When further equipped with linking and rescoring, we obtain to-date the best reported mAP of 83.8% and 84.7%.",
20,2019,http://arxiv.org/abs/1903.01522,TKD: Temporal Knowledge Distillation for Active Perception,"Farhadi, Mohammad and Yang, Yezhou",Farhadi2019,,,,1903.01522,arXiv,"Deep neural networks based methods have been proved to achieve outstanding performance on object detection and classification tasks. Despite significant performance improvement, due to the deep structures, they still require prohibitive runtime to process images and maintain the highest possible performance for real-time applications. Observing the phenomenon that human vision system (HVS) relies heavily on the temporal dependencies among frames from the visual input to conduct recognition efficiently, we propose a novel framework dubbed as TKD: temporal knowledge distillation. This framework distills the temporal knowledge from a heavy neural networks based model over selected video frames (the perception of the moments) to a light-weight model. To enable the distillation, we put forward two novel procedures: 1) an Long-short Term Memory (LSTM) based key frame selection method; and 2) a novel teacher-bounded loss design. To validate, we conduct comprehensive empirical evaluations using different object detection methods over multiple datasets including Youtube-Objects and Hollywood scene dataset. Our results show consistent improvement in accuracy-speed trad-offs for object detection over the frames of the dynamic scene, compare to other modern object recognition methods.",undefined
22,2019,http://arxiv.org/abs/1905.02161,Batch Normalization is a Cause of Adversarial Vulnerability,"Galloway, Angus and Golubeva, Anna and Tanay, Thomas and Moussa, Medhat and Taylor, Graham W.",Galloway2019,may,,,1905.02161,arXiv,"Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on five standard datasets. Furthermore, substituting weight decay for batch norm is sufficient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-field analysis that found that batch norm causes exploding gradients.",
25,2019,http://arxiv.org/abs/1901.09244,DistInit: Learning Video Representations Without a Single Labeled Video,"Girdhar, Rohit and Tran, Du and Torresani, Lorenzo and Ramanan, Deva",Girdhar2019,jan,,,1901.09244,arXiv,"Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models have not been able to keep up with the ever-increasing depth and sophistication of these networks. In this work, we propose an alternative approach to learning video representations that require no semantically labeled videos and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as ""teachers"" to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image-based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.",
28,2019,https://ieeexplore.ieee.org/document/8756937/,Distillation of a CNN for a high accuracy mobile face recognition system,"Guzzi, Francesco and Bortoli, Luca De and Marsi, Stefano and Carrato, Sergio and Ramponi, Giovanni",Guzzi2019,may,IEEE,10.23919/MIPRO.2019.8756937,,,,
31,2019,http://arxiv.org/abs/1904.01866,A Comprehensive Overhaul of Feature Distillation,"Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young",Heo2019,apr,,,1904.01866,arXiv,"We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at https://sites.google.com/view/byeongho-heo/overhaul",
34,2019,http://arxiv.org/abs/1906.08467,GAN-Knowledge Distillation for one-stage Object Detection,"Hong, Wei and Zong, Jin ke Yu Fan",Hong2019,jun,,,1906.08467,arXiv,"Convolutional neural networks have a significant improvement in the accuracy of Object detection. As convolutional neural networks become deeper, the accuracy of detection is also obviously improved, and more floating-point calculations are needed. Many researchers use the knowledge distillation method to improve the accuracy of student networks by transferring knowledge from a deeper and larger teachers network to a small student network, in object detection. Most methods of knowledge distillation need to designed complex cost functions and they are aimed at the two-stage object detection algorithm. This paper proposes a clean and effective knowledge distillation method for the one-stage object detection. The feature maps generated by teacher network and student network are used as true samples and fake samples respectively, and generate adversarial training for both to improve the performance of the student network in one-stage object detection.",
47,2019,https://www.mdpi.com/2076-3417/9/10/1966,Layer-Level Knowledge Distillation for Deep Neural Network Learning,"Li, Hao-Ting and Lin, Shih-Chieh and Chen, Cheng-Yeh and Chiang, Chen-Kuo and Li, Hao-Ting and Lin, Shih-Chieh and Chen, Cheng-Yeh and Chiang, Chen-Kuo",Li2019,may,Multidisciplinary Digital Publishing Institute,10.3390/app9101966,,,"\textlessp\textgreaterMotivated by the recently developed distillation approaches that aim to obtain small and fast-to-execute models, in this paper a novel Layer Selectivity Learning (LSL) framework is proposed for learning deep models. We firstly use an asymmetric dual-model learning framework, called Auxiliary Structure Learning (ASL), to train a small model with the help of a larger and well-trained model. Then, the intermediate layer selection scheme, called the Layer Selectivity Procedure (LSP), is exploited to determine the corresponding intermediate layers of source and target models. The LSP is achieved by two novel matrices, the layered inter-class Gram matrix and the inter-layered Gram matrix, to evaluate the diversity and discrimination of feature maps. The experimental results, demonstrated using three publicly available datasets, present the superior performance of model training using the LSL deep model learning framework.\textless/p\textgreater",Applied Sciences
52,2019,http://arxiv.org/abs/1904.03249,Paying More Attention to Motion: Attention Distillation for Learning Video Representations,"Liu, Miao and Chen, Xin and Zhang, Yun and Li, Yin and Rehg, James M.",Liu2019,apr,,,1904.03249,arXiv,"We address the challenging problem of learning motion representations using deep models for video recognition. To this end, we make use of attention modules that learn to highlight regions in the video and aggregate features for recognition. Specifically, we propose to leverage output attention maps as a vehicle to transfer the learned representation from a motion (flow) network to an RGB network. We systematically study the design of attention modules, and develop a novel method for attention distillation. Our method is evaluated on major action benchmarks, and consistently improves the performance of the baseline RGB network by a significant margin. Moreover, we demonstrate that our attention maps can leverage motion cues in learning to identify the location of actions in video frames. We believe our method provides a step towards learning motion-aware representations in deep models.",
54,2019,http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html,Structured Knowledge Distillation for Semantic Segmentation,"Liu, Yifan and Chen, Ke and Liu, Chris and Qin, Zengchang and Luo, Zhenbo and Wang, Jingdong",Liu2019a,,,,,,,
107,2019,https://www.semanticscholar.org/paper/Be-Your-Own-Teacher%3A-Improve-the-Performance-of-via-Zhang-Song/a8cab29d2230924dffe89d6dda15ba42790c5ebf,Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation,"Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng",Zhang2019,,,,,,,undefined
63,2019,https://www.semanticscholar.org/paper/Zero-shot-Knowledge-Transfer-via-Adversarial-Belief-Micaelli-Storkey/6b67db233caa84b42b934556bc5b186f0ef7a510,Zero-shot Knowledge Transfer via Adversarial Belief Matching,"Micaelli, Paul and Storkey, Amos J.",Micaelli2019,,,,,,,undefined
68,2019,http://arxiv.org/abs/1909.01058,Knowledge Distillation for End-to-End Person Search,"Munjal, Bharti and Galasso, Fabio and Amin, Sikandar",Munjal2019,sep,,,1909.01058,arXiv,"We introduce knowledge distillation for end-to-end person search. End-to-End methods are the current state-of-the-art for person search that solve both detection and re-identification jointly. These approaches for joint optimization show their largest drop in performance due to a sub-optimal detector. We propose two distinct approaches for extra supervision of end-to-end person search methods in a teacher-student setting. The first is adopted from state-of-the-art knowledge distillation in object detection. We employ this to supervise the detector of our person search model at various levels using a specialized detector. The second approach is new, simple and yet considerably more effective. This distills knowledge from a teacher re-identification technique via a pre-computed look-up table of ID features. It relaxes the learning of identification features and allows the student to focus on the detection task. This procedure not only helps fixing the sub-optimal detector training in the joint optimization and simultaneously improving the person search, but also closes the performance gap between the teacher and the student for model compression in this case. Overall, we demonstrate significant improvements for two recent state-of-the-art methods using our proposed knowledge distillation approach on two benchmark datasets. Moreover, on the model compression task our approach brings the performance of smaller models on par with the larger models.",
69,2019,https://www.semanticscholar.org/paper/Zero-Shot-Knowledge-Distillation-in-Deep-Networks-Nayak-Mopuri/eced39bd4b6aa1097a7615dd0c502e4a7510796a,Zero-Shot Knowledge Distillation in Deep Networks,"Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Radhakrishnan, Venkatesh Babu and Chakraborty, Anirban",Nayak2019,,,,,,,undefined
72,2019,https://www.sciencedirect.com/science/article/pii/S0893608019300231,Continual lifelong learning with neural networks: A review,"Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan",Parisi2019,may,Pergamon,10.1016/J.NEUNET.2019.01.012,,,"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",Neural Networks
77,2019,http://arxiv.org/abs/1903.11752,ThunderNet: Towards Real-time Generic Object Detection,"Qin, Zheng and Li, Zeming and Zhang, Zhaoning and Bao, Yiping and Yu, Gang and Peng, Yuxing and Sun, Jian",Qin2019,,,,1903.11752,arXiv,"Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. However, previous CNN-based detectors suffer from enormous computational cost, which hinders them from real-time inference in computation-constrained scenarios. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Compared with lightweight one-stage detectors, ThunderNet achieves superior performance with only 40% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps on an ARM-based device. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Code will be released for paper reproduction.",undefined
108,2019,https://www.semanticscholar.org/paper/Highlight-Every-Step%3A-Knowledge-Distillation-via-Zhao-Sun/43ec0125daa51cb79daa6471f24ed71964820762,Highlight Every Step : Improved Knowledge Distillation Method via Collaborative Teaching,"Zhao, Haoran and Sun, Xin and Dong, Junyu and Chen, Changrui and Dong, Zihe",Zhao2019,,,,,,,
18,,https://papers.nips.cc/paper/7553-moonshine-distilling-with-cheap-convolutions.pdf,Moonshine: Distilling with Cheap Convolutions,"Crowley, Elliot J and Gray, Gavin and Storkey, Amos",Crowley,,,,,,"Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.",
26,,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gupta_Cross_Modal_Distillation_CVPR_2016_paper.pdf,Cross Modal Distillation for Supervision Transfer,"Gupta, Saurabh and Hoffman, Judy and Malik, Jitendra",Gupta,,,,,,In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers.,
27,,https://arxiv.org/pdf/1809.10562v1.pdf,Dropout Distillation for Efficiently Estimating Model Confidence,"Gurau, Corina and Bewley, Alex and Posner, Ingmar",Gurau,,,,1809.10562v1,arXiv,"We propose an efficient way to output better calibrated uncertainty scores from neural networks. The Distilled Dropout Network (DDN) makes standard (non-Bayesian) neural networks more introspective by adding a new training loss which prevents them from being overconfident. Our method is more efficient than Bayesian neural networks or model ensembles which, despite providing more reliable uncertainty scores, are more cumbersome to train and slower to test. We evaluate DDN on the the task of image classification on the CIFAR-10 dataset and show that our calibration results are competitive even when compared to 100 Monte Carlo samples from a dropout network while they also increase the classification accuracy. We also propose better calibration within the state of the art Faster R-CNN object detection framework and show, using the COCO dataset, that DDN helps train better calibrated object detectors.",
35,,https://www.aclweb.org/anthology/D16-1173,Deep Neural Networks with Massive Learned Knowledge,"Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P",Hu,,,,,,"Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability. We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts. We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures. Our model substantially enhances the performance using less training data, and shows improved inter-pretability. The principled framework can also be applied to posterior regularization for regulating other statistical models.",
39,,https://arxiv.org/pdf/1802.04977.pdf,Paraphrasing Complex Network: Network Compression via Factor Transfer,"Kim, Jangho and Park, Seonguk and Kwak, Nojun",Kim,,,,1802.04977v2,arXiv,"Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.",
41,,http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf,Bayesian Dark Knowledge,"Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Research, Google and Welling, Max",Korattikara,,,,,,"We consider the problem of Bayesian parameter estimation for deep neural networks , which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x, D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for ""distilling"" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.",
62,,https://arxiv.org/pdf/1805.06361.pdf,Object detection at 200 Frames Per Second,"Mehta, Rakesh and Ozturk, Cemalettin",Mehta,,,,1805.06361v1,arXiv,"In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture, loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally lightweight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed lightweight student network. We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset.",
76,,http://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf,3D Graph Neural Networks for RGBD Semantic Segmentation,"Qi, Xiaojuan and Liao, Renjie and Jia, Jiaya and Fidler, Sanja and Urtasun, Raquel",Qi,,,,,,"RGBD semantic segmentation requires joint reasoning about 2D appearance and 3D geometric information. In this paper we propose a 3D graph neural network (3DGNN) that builds a k-nearest neighbor graph on top of 3D point cloud. Each node in the graph corresponds to a set of points and is associated with a hidden representation vector ini-tialized with an appearance feature extracted by a unary CNN from 2D images. Relying on recurrent functions, every node dynamically updates its hidden representation based on the current status and incoming messages from its neigh-bors. This propagation model is unrolled for a certain num-ber of time steps and the final per-node representation is used for predicting the semantic class of each pixel. We use back-propagation through time to train the model. Ex-tensive experiments on NYUD2 and SUN-RGBD datasets demonstrate the effectiveness of our approach.",
79,,http://teaching-machines.cc/nips2017/papers/nips17-teaching_paper-13.pdf,Generative Knowledge Distillation for General Purpose Function Compression,"Riemer, Matthew and Franceschini, Michele and Bouneffouf, Djallel and Klinger, Tim",Riemer,,,,,,"Deep lifelong learning systems need to efficiently manage resources to scale to large numbers of experiences and non-stationary goals. In this paper, we explore the relationship between lossy compression and the resource constrained lifelong learning problem of function transferability. We demonstrate that lossy episodic experience storage can enable efficient function transferability between different architectures and algorithms at a fraction of the storage cost of lossless storage. This is achieved by introducing a generative knowledge distillation strategy that does not store any full training examples.",
